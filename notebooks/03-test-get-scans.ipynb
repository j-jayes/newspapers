{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12794f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f96a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8e8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_driver(download_dir=\"newspaper_downloads\"):\n",
    "    \"\"\"\n",
    "    Initializes the download directory and sets up the Selenium Chrome driver.\n",
    "    \n",
    "    Returns:\n",
    "        driver: Selenium webdriver instance.\n",
    "        wait: WebDriverWait instance for the driver.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run headless if you don't need to see the browser\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    return driver, wait\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04ed30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_jp2_from_manifest_data(manifest_url):\n",
    "    \"\"\"\n",
    "    Extract JP2 file URLs directly from the manifest data.\n",
    "    \n",
    "    Args:\n",
    "        manifest_url (str): The URL for the manifest.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of JP2 file URLs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching manifest data from: {manifest_url}\")\n",
    "        headers = {\n",
    "            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                           'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                           'Chrome/91.0.4472.124 Safari/537.36'),\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Referer': 'https://tidningar.kb.se/',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(f\"{manifest_url}/manifest\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        manifest_data = response.json()\n",
    "        jp2_urls = []\n",
    "        \n",
    "        # Extract JP2 URLs from the manifest items\n",
    "        if 'items' in manifest_data:\n",
    "            for canvas in manifest_data['items']:\n",
    "                if 'items' in canvas:\n",
    "                    for annotation_page in canvas['items']:\n",
    "                        if 'items' in annotation_page:\n",
    "                            for annotation in annotation_page['items']:\n",
    "                                if 'body' in annotation and 'id' in annotation['body']:\n",
    "                                    body_id = annotation['body']['id']\n",
    "                                    if body_id.endswith('.jp2'):\n",
    "                                        jp2_urls.append(body_id)\n",
    "        \n",
    "        return jp2_urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JP2 files from manifest: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def download_file(url, filepath):\n",
    "    \"\"\"\n",
    "    Download a file from URL to the specified filepath.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the file to download.\n",
    "        filepath (str): Destination file path.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if download is successful, otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading {url} to {filepath}\")\n",
    "        \n",
    "        # Skip download if file already exists\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"File already exists: {filepath}\")\n",
    "            return True\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                           'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                           'Chrome/91.0.4472.124 Safari/537.36'),\n",
    "            'Referer': 'https://tidningar.kb.se/',\n",
    "            'Accept': 'image/jpeg, image/png, image/jp2, */*'\n",
    "        }\n",
    "        \n",
    "        # Clean the URL in case of malformed characters\n",
    "        clean_url = url.replace('\\\\\\\\', '/')\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(clean_url, headers=headers, stream=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Create the directory if it doesn't exist\n",
    "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                \n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {filepath}\")\n",
    "                return True\n",
    "                \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.Timeout) as req_err:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Retry {attempt+1}/{max_retries} downloading {clean_url}: {req_err}\")\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "                else:\n",
    "                    raise\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e64773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:106: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:106: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/var/folders/xp/90b2vmmj0cg7rv097x887f6m0000gn/T/ipykernel_61293/4170192347.py:106: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  jp2_urls = [url.replace('\\/', '/') for url in jp2_matches]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_by_date_range(driver, wait, download_dir, start_date, end_date, paper_id=None):\n",
    "    \"\"\"\n",
    "    Scrape newspapers within a given date range.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium webdriver instance.\n",
    "        wait: WebDriverWait instance.\n",
    "        download_dir (str): Base directory to save downloads.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "        paper_id (str, optional): Optional paper ID filter.\n",
    "    \"\"\"\n",
    "    base_url = \"https://tidningar.kb.se/search?q=%2a\"\n",
    "    url = f\"{base_url}&from={start_date}&to={end_date}\"\n",
    "    \n",
    "    # Add paper filter if provided, otherwise use default\n",
    "    if paper_id:\n",
    "        url += f\"&isPartOf.%40id={paper_id}\"\n",
    "    else:\n",
    "        url += \"&isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2Fm5z2w4lz3m2zxpk%23it\"\n",
    "    \n",
    "    print(f\"Using search URL: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Allow page to load\n",
    "\n",
    "    # Find and process each newspaper result\n",
    "    results = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.search-result-item\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(results)} newspaper issues\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            # Extract date and title before clicking\n",
    "            newspaper_title = None\n",
    "            newspaper_date = None\n",
    "            \n",
    "            date_elements = result.find_elements(By.CSS_SELECTOR, \"div.date-text\")\n",
    "            if date_elements:\n",
    "                newspaper_date = date_elements[0].text.strip()\n",
    "            \n",
    "            title_elements = result.find_elements(By.CSS_SELECTOR, \"div.search-result-title\")\n",
    "            if title_elements:\n",
    "                newspaper_title = title_elements[0].text.strip()\n",
    "            \n",
    "            print(f\"Processing issue {i+1}/{len(results)}: {newspaper_title} - {newspaper_date}\")\n",
    "            \n",
    "            # Click on the result to open the newspaper issue\n",
    "            result.click()\n",
    "            time.sleep(3)  # Wait for the issue page to load\n",
    "            \n",
    "            current_url = driver.current_url\n",
    "            \n",
    "            try:\n",
    "                print(\"Extracting manifest data from page source\")\n",
    "                page_source = driver.page_source\n",
    "                manifest_pattern = r'\"id\":\"(https:\\\\\\/\\\\\\/data\\.kb\\.se\\\\\\/([^\\/\\\\]+)\\\\\\/manifest)\"'\n",
    "                match = re.search(manifest_pattern, page_source)\n",
    "                \n",
    "                if match:\n",
    "                    manifest_id = match.group(2)  # Extract the ID portion\n",
    "                    print(f\"Found manifest ID: {manifest_id}\")\n",
    "                    manifest_url = f\"https://data.kb.se/{manifest_id}\"\n",
    "                else:\n",
    "                    # Alternative extraction method from URL path\n",
    "                    parsed_url = urlparse(current_url)\n",
    "                    path_parts = parsed_url.path.split('/')\n",
    "                    if len(path_parts) > 1:\n",
    "                        manifest_id = path_parts[1]\n",
    "                        print(f\"Extracted manifest ID from URL: {manifest_id}\")\n",
    "                        manifest_url = f\"https://data.kb.se/{manifest_id}\"\n",
    "                    else:\n",
    "                        raise Exception(\"Could not extract manifest ID\")\n",
    "                    \n",
    "                    # Navigate to the manifest page\n",
    "                    driver.get(manifest_url)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # If newspaper info is missing, attempt to extract from the page title\n",
    "                    if not newspaper_date or not newspaper_title:\n",
    "                        page_title = driver.title\n",
    "                        title_match = re.search(r'([^|]+)', page_title)\n",
    "                        if title_match:\n",
    "                            combined_title = title_match.group(1).strip()\n",
    "                            parts = combined_title.split()\n",
    "                            if len(parts) >= 2:\n",
    "                                newspaper_title = ' '.join(parts[:-1])\n",
    "                                newspaper_date = parts[-1]\n",
    "                    \n",
    "                    # Clean newspaper_title for folder naming\n",
    "                    newspaper_title = re.sub(r'[^\\w\\s-]', '', newspaper_title).strip() if newspaper_title else \"Unknown\"\n",
    "                    newspaper_date = newspaper_date.replace('/', '-') if newspaper_date else \"Unknown_Date\"\n",
    "                    \n",
    "                    # Create folder for this newspaper issue\n",
    "                    folder_path = os.path.join(download_dir, newspaper_title, newspaper_date)\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "                    \n",
    "                    # Try different methods to extract JP2 file URLs\n",
    "                    \n",
    "                    # Method 1: Extract from page source JSON data\n",
    "                    page_source = driver.page_source\n",
    "                    jp2_pattern = r'\"id\":\"(https:\\\\\\/\\\\\\/data\\.kb\\.se\\\\\\/[^\"]+\\.jp2)\"'\n",
    "                    jp2_matches = re.findall(jp2_pattern, page_source)\n",
    "                    jp2_urls = [url.replace('\\/', '/') for url in jp2_matches]\n",
    "                    \n",
    "                    print(f\"Method 1: Found {len(jp2_urls)} JP2 files in page source\")\n",
    "                    \n",
    "                    # Method 2: Use the manifest data extraction function if needed\n",
    "                    if not jp2_urls:\n",
    "                        jp2_urls = extract_jp2_from_manifest_data(manifest_url)\n",
    "                        print(f\"Method 2: Found {len(jp2_urls)} JP2 files from manifest data\")\n",
    "                    \n",
    "                    # Method 3: Look for links on the page if still nothing\n",
    "                    if not jp2_urls:\n",
    "                        jp2_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='.jp2']\")\n",
    "                        if jp2_links:\n",
    "                            jp2_urls = [link.get_attribute('href') for link in jp2_links]\n",
    "                            print(f\"Method 3: Found {len(jp2_urls)} JP2 files from page links\")\n",
    "                    \n",
    "                    # Download each JP2 file\n",
    "                    for file_url in jp2_urls:\n",
    "                        # Clean up the URL formatting\n",
    "                        if '\\\\' in file_url:\n",
    "                            file_url = file_url.replace('\\\\', '')\n",
    "                        filename = os.path.basename(unquote(file_url))\n",
    "                        file_path = os.path.join(folder_path, filename)\n",
    "                        download_file(file_url, file_path)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing manifest: {e}\")\n",
    "            \n",
    "            # Return to the search results\n",
    "            driver.back()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # If necessary, re-navigate to the search page\n",
    "            if \"search\" not in driver.current_url:\n",
    "                driver.get(url)\n",
    "                time.sleep(3)\n",
    "            \n",
    "            # Refresh the results list\n",
    "            results = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.search-result-item\"))\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing newspaper issue: {e}\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            results = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.search-result-item\"))\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9dced94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using search URL: https://tidningar.kb.se/search?q=%2a&from=1865-01-02&to=1865-01-03&isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2Fm5z2w4lz3m2zxpk%23it\n",
      "Found 2 newspaper issues\n",
      "Processing issue 1/2: None - None\n",
      "Extracting manifest data from page source\n",
      "Extracted manifest ID from URL: l4x2vhvx1jj0mch\n",
      "Method 1: Found 0 JP2 files in page source\n",
      "Fetching manifest data from: https://data.kb.se/l4x2vhvx1jj0mch\n",
      "Error extracting JP2 files from manifest: 404 Client Error: Not Found for url: https://data.kb.se/l4x2vhvx1jj0mch/manifest\n",
      "Method 2: Found 0 JP2 files from manifest data\n",
      "Error processing newspaper issue: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=134.0.6998.89); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100631804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000100629ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x000000010017dea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x000000010018e7d4 cxxbridge1$string$len + 160812\n",
      "4   chromedriver                        0x000000010018d888 cxxbridge1$string$len + 156896\n",
      "5   chromedriver                        0x0000000100184458 cxxbridge1$string$len + 118960\n",
      "6   chromedriver                        0x0000000100182b64 cxxbridge1$string$len + 112572\n",
      "7   chromedriver                        0x0000000100185d74 cxxbridge1$string$len + 125388\n",
      "8   chromedriver                        0x0000000100185e1c cxxbridge1$string$len + 125556\n",
      "9   chromedriver                        0x00000001001c4d9c cxxbridge1$string$len + 383476\n",
      "10  chromedriver                        0x00000001001ba878 cxxbridge1$string$len + 341200\n",
      "11  chromedriver                        0x0000000100206678 cxxbridge1$string$len + 651984\n",
      "12  chromedriver                        0x00000001001b935c cxxbridge1$string$len + 335796\n",
      "13  chromedriver                        0x00000001005f6cd4 cxxbridge1$str$ptr + 2545532\n",
      "14  chromedriver                        0x00000001005f9fa0 cxxbridge1$str$ptr + 2558536\n",
      "15  chromedriver                        0x00000001005d6d04 cxxbridge1$str$ptr + 2414508\n",
      "16  chromedriver                        0x00000001005fa800 cxxbridge1$str$ptr + 2560680\n",
      "17  chromedriver                        0x00000001005c7ba0 cxxbridge1$str$ptr + 2352712\n",
      "18  chromedriver                        0x000000010061a45c cxxbridge1$str$ptr + 2690820\n",
      "19  chromedriver                        0x000000010061a5e4 cxxbridge1$str$ptr + 2691212\n",
      "20  chromedriver                        0x0000000100629a50 cxxbridge1$str$ptr + 2753784\n",
      "21  libsystem_pthread.dylib             0x0000000190a282e4 _pthread_start + 136\n",
      "22  libsystem_pthread.dylib             0x0000000190a230fc thread_start + 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage in a notebook:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the driver and create the download directory \"kb_newspapers\"\n",
    "    driver, wait = init_driver(download_dir=\"kb_newspapers\")\n",
    "    \n",
    "    try:\n",
    "        # Test the scraper for newspapers between 1865-01-02 and 1865-01-03\n",
    "        scrape_by_date_range(driver, wait, \"kb_newspapers\", \"1865-01-02\", \"1865-01-03\")\n",
    "    finally:\n",
    "        # Close the driver when done\n",
    "        driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
