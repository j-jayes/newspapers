{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340f7c70",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Ingest the information on which newspapers are available in the database at [data.kb.se](https://data.kb.se/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f120dfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1948 newspapers\n",
      "                                                name  \\\n",
      "0                                            8 sidor   \n",
      "1  Academiska och stifts tidningar utg. i Lund fö...   \n",
      "2                              Adress- & varutidning   \n",
      "3                    Adress- hyres- och annonsbladet   \n",
      "4    Adress-kontor för Östergötland med Wadstena län   \n",
      "\n",
      "                                                 url pages  \n",
      "0  https://tidningar.kb.se/search?isPartOf.%40id=...  4692  \n",
      "1  https://tidningar.kb.se/search?isPartOf.%40id=...   200  \n",
      "2  https://tidningar.kb.se/search?isPartOf.%40id=...    54  \n",
      "3  https://tidningar.kb.se/search?isPartOf.%40id=...    80  \n",
      "4  https://tidningar.kb.se/search?isPartOf.%40id=...   174  \n",
      "Data saved to ../data/kb_newspapers.csv\n",
      "Data also saved as JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Fix the URL - add the missing 'h'\n",
    "url = \"https://tidningar.kb.se/titles\"\n",
    "\n",
    "# Get the data from the KB website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all table rows containing newspaper information\n",
    "    table_rows = soup.select('tr')\n",
    "    \n",
    "    # Create lists to store the newspaper names, their URLs, and page counts\n",
    "    newspaper_names = []\n",
    "    newspaper_urls = []\n",
    "    newspaper_pages = []\n",
    "    \n",
    "    # Extract the names, URLs, and page counts\n",
    "    for row in table_rows:\n",
    "        # Look for a link in the first cell\n",
    "        link = row.select_one('td a')\n",
    "        if link:\n",
    "            # Get the newspaper name from the link text\n",
    "            name = link.text.strip()\n",
    "            \n",
    "            # Get the URL from the href attribute\n",
    "            url_value = link.get('href', '')\n",
    "            if url_value.startswith('/'):\n",
    "                url_value = 'https://tidningar.kb.se' + url_value\n",
    "            \n",
    "            # Get the page count from the second cell\n",
    "            page_count_cell = row.select_one('td.text-right span')\n",
    "            if page_count_cell:\n",
    "                # Remove non-breaking spaces and other whitespace\n",
    "                page_count = page_count_cell.text.strip().replace('\\xa0', '').replace(' ', '')\n",
    "            else:\n",
    "                page_count = \"Unknown\"\n",
    "            \n",
    "            newspaper_names.append(name)\n",
    "            newspaper_urls.append(url_value)\n",
    "            newspaper_pages.append(page_count)\n",
    "    \n",
    "    # Create a DataFrame to store the data\n",
    "    newspapers_df = pd.DataFrame({\n",
    "        'name': newspaper_names,\n",
    "        'url': newspaper_urls,\n",
    "        'pages': newspaper_pages\n",
    "    })\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(f\"Found {len(newspapers_df)} newspapers\")\n",
    "    print(newspapers_df.head())\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    output_path = \"../data/kb_newspapers.csv\"\n",
    "    newspapers_df.to_csv(output_path, index=False)\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "    \n",
    "    # Save as JSON as well for more structured data\n",
    "    with open(\"../data/kb_newspapers.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(newspapers_df.to_dict(orient='records'), json_file, ensure_ascii=False, indent=4)\n",
    "    print(\"Data also saved as JSON\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f30511",
   "metadata": {},
   "source": [
    "In the next step, we need to loop through the URLs and find out how many issues there are per year, which is shown on the page with the following style of histogram:\n",
    "\n",
    "where there is a bar for each year, and the data label is the year and data value is the number of issues.\n",
    "\n",
    "```html\n",
    "\n",
    "<rect data-v-4a5ad4a2=\"\" data-label=\"1899\" data-value=\"4\" height=\"4.19047619047619\" width=\"57.77777777777778\" x=\"0\" y=\"120.80952380952381\" class=\"bar--filled\"></rect>\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This sits inside the `histogram` class:\n",
    "\n",
    "```html\n",
    "\n",
    "<svg data-v-4a5ad4a2=\"\" width=\"600\" height=\"145\" xmlns=\"http://www.w3.org/2000/svg\" class=\"histogram\"><g data-v-4a5ad4a2=\"\" transform=\"translate(60, 0)\" class=\"main\"><g data-v-4a5ad4a2=\"\" fill=\"none\" transform=\"translate(0, 0)\" class=\"y-axis\" style=\"color: rgb(204, 204, 204);\"><path data-v-4a5ad4a2=\"\" stroke=\"currentColor\" d=\"M0.5,125.0H0.5V0.5\" class=\"domain\"></path> <g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"end\" transform=\"translate(0, 125.5)\" class=\"tick\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" x2=\"-6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" x=\"-9\" dy=\"0.32em\">0</text></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"end\" transform=\"translate(0, 73.11904761904762)\" class=\"tick\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" x2=\"-6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" x=\"-9\" dy=\"0.32em\">50</text></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"end\" transform=\"translate(0, 20.738095238095244)\" class=\"tick\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" x2=\"-6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" x=\"-9\" dy=\"0.32em\">100</text></g></g> <g data-v-4a5ad4a2=\"\" fill=\"none\" transform=\"translate(0, 125)\" class=\"x-axis\" style=\"color: rgb(204, 204, 204);\"><path data-v-4a5ad4a2=\"\" stroke=\"currentColor\" d=\"M0,0.5H520.5\" class=\"domain\"></path> <g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(35.30864197530863, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1899</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(99.50617283950616, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1900</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(163.7037037037037, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1901</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(227.90123456790124, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1902</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(292.0987654320988, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1903</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(356.2962962962963, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1904</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(420.4938271604939, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1905</text></g></g><g data-v-4a5ad4a2=\"\" opacity=\"1\" font-size=\"12\" font-family=\"sans-serif\" text-anchor=\"middle\" transform=\"translate(484.6913580246914, 0)\" class=\"tick\"><g data-v-4a5ad4a2=\"\"><line data-v-4a5ad4a2=\"\" stroke=\"currentColor\" y2=\"6\"></line> <text data-v-4a5ad4a2=\"\" fill=\"#000\" y=\"9\" dy=\"0.71em\">1906</text></g></g></g> <g data-v-4a5ad4a2=\"\" fill=\"none\" class=\"bars bars--wide\" style=\"position: relative;\"><rect data-v-4a5ad4a2=\"\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"520\" x=\"0\" y=\"0\"></rect> <g data-v-4a5ad4a2=\"\" data-return-value=\"1899\" transform=\"translate(6.419753086419746,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1899\" data-value=\"4\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1899\" data-value=\"4\" height=\"4.19047619047619\" width=\"57.77777777777778\" x=\"0\" y=\"120.80952380952381\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1900\" transform=\"translate(70.61728395061728,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1900\" data-value=\"103\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1900\" data-value=\"103\" height=\"107.9047619047619\" width=\"57.77777777777778\" x=\"0\" y=\"17.0952380952381\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1901\" transform=\"translate(134.8148148148148,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1901\" data-value=\"104\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1901\" data-value=\"104\" height=\"108.95238095238096\" width=\"57.77777777777778\" x=\"0\" y=\"16.047619047619044\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1902\" transform=\"translate(199.01234567901236,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1902\" data-value=\"103\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1902\" data-value=\"103\" height=\"107.9047619047619\" width=\"57.77777777777778\" x=\"0\" y=\"17.0952380952381\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1903\" transform=\"translate(263.2098765432099,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1903\" data-value=\"105\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1903\" data-value=\"105\" height=\"110\" width=\"57.77777777777778\" x=\"0\" y=\"15\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1904\" transform=\"translate(327.4074074074074,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1904\" data-value=\"104\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1904\" data-value=\"104\" height=\"108.95238095238096\" width=\"57.77777777777778\" x=\"0\" y=\"16.047619047619044\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1905\" transform=\"translate(391.60493827160496,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1905\" data-value=\"104\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1905\" data-value=\"104\" height=\"108.95238095238096\" width=\"57.77777777777778\" x=\"0\" y=\"16.047619047619044\" class=\"bar--filled\"></rect></g><g data-v-4a5ad4a2=\"\" data-return-value=\"1906\" transform=\"translate(455.8024691358025,0)\" class=\"bar ds-selectable\"><rect data-v-4a5ad4a2=\"\" data-label=\"1906\" data-value=\"104\" fill=\"#000\" fill-opacity=\"0\" height=\"125\" width=\"57.77777777777778\" x=\"0\" y=\"0\"></rect> <rect data-v-4a5ad4a2=\"\" data-label=\"1906\" data-value=\"104\" height=\"108.95238095238096\" width=\"57.77777777777778\" x=\"0\" y=\"16.047619047619044\" class=\"bar--filled\"></rect></g></g> <g data-v-4a5ad4a2=\"\"><!----></g></g></svg>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141681ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing #1: 8 sidor\n",
      "Processing URL: https://tidningar.kb.se/search?isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2Fwf7fbxb71333dp5%23it\n",
      "Found 12 years with issues\n",
      "\n",
      "Processing #2: Academiska och stifts tidningar utg. i Lund för år 1773 af G.S.\n",
      "Processing URL: https://tidningar.kb.se/search?isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2F08667xhcx8zwx7mj%23it\n",
      "Found 1 years with issues\n",
      "\n",
      "Processing #3: Adress- & varutidning\n",
      "Processing URL: https://tidningar.kb.se/search?isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2F4fr1qj1x2ddfhktc%23it\n",
      "Found 2 years with issues\n",
      "\n",
      "Saved detailed information for 3 newspapers to ../data/kb_newspapers_with_years_test.json\n",
      "\n",
      "Newspaper: 8 sidor\n",
      "Total pages: 4692\n",
      "Years with issues: 12\n",
      "Sample years (year: issue count):\n",
      "  2013: 49\n",
      "  2014: 48\n",
      "  2015: 49\n",
      "  2016: 49\n",
      "  2017: 49\n",
      "\n",
      "Newspaper: Academiska och stifts tidningar utg. i Lund för år 1773 af G.S.\n",
      "Total pages: 200\n",
      "Years with issues: 1\n",
      "Sample years (year: issue count):\n",
      "  1773: 50\n",
      "\n",
      "Newspaper: Adress- & varutidning\n",
      "Total pages: 54\n",
      "Years with issues: 2\n",
      "Sample years (year: issue count):\n",
      "  1904: 21\n",
      "  1905: 6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Load the previously saved newspaper data\n",
    "newspapers_df = pd.read_csv(\"../data/kb_newspapers.csv\")\n",
    "\n",
    "def get_issues_by_year(url):\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        # Set up Selenium with headless Chrome\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        # Automatically install/update ChromeDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        # Visit the URL and wait for JavaScript to load the SVG\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Increase the delay if needed\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the rendered HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find the histogram SVG\n",
    "        histogram = soup.select_one('svg.histogram')\n",
    "        if not histogram:\n",
    "            print(f\"No histogram found for {url}\")\n",
    "            return {}\n",
    "        \n",
    "        # Extract the year and issue count from the bars in the histogram\n",
    "        issues_by_year = {}\n",
    "        bar_groups = histogram.select('g.bar.ds-selectable')\n",
    "        \n",
    "        for bar_group in bar_groups:\n",
    "            # Get the year from the data-return-value attribute\n",
    "            year = bar_group.get('data-return-value')\n",
    "            # Find the filled rect within the bar group\n",
    "            filled_rect = bar_group.select_one('rect.bar--filled')\n",
    "            if filled_rect:\n",
    "                count = filled_rect.get('data-value')\n",
    "                if year and count:\n",
    "                    issues_by_year[year] = int(count)\n",
    "        \n",
    "        return issues_by_year\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Process only the first 3 newspapers as a test\n",
    "test_newspapers = newspapers_df.head(3)\n",
    "results = []\n",
    "\n",
    "for idx, row in test_newspapers.iterrows():\n",
    "    newspaper_name = row['name']\n",
    "    newspaper_url = row['url']\n",
    "    total_pages = row['pages']\n",
    "    \n",
    "    print(f\"\\nProcessing #{idx+1}: {newspaper_name}\")\n",
    "    issues_by_year = get_issues_by_year(newspaper_url)\n",
    "    \n",
    "    result = {\n",
    "        'name': newspaper_name,\n",
    "        'url': newspaper_url,\n",
    "        'total_pages': total_pages,\n",
    "        'issues_by_year': issues_by_year\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    print(f\"Found {len(issues_by_year)} years with issues\")\n",
    "\n",
    "# Save the detailed results to a JSON file\n",
    "output_path = \"../data/kb_newspapers_with_years_test.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"\\nSaved detailed information for {len(results)} newspapers to {output_path}\")\n",
    "\n",
    "# Display a sample of the results\n",
    "for result in results:\n",
    "    print(f\"\\nNewspaper: {result['name']}\")\n",
    "    print(f\"Total pages: {result['total_pages']}\")\n",
    "    print(f\"Years with issues: {len(result['issues_by_year'])}\")\n",
    "    # Show up to 5 years as examples\n",
    "    sample_years = list(result['issues_by_year'].items())[:5]\n",
    "    if sample_years:\n",
    "        print(\"Sample years (year: issue count):\")\n",
    "        for year, count in sample_years:\n",
    "            print(f\"  {year}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
